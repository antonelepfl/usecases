{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Only Python 3 compatible! Python 2 support has been dropped since v3.0 of this UseCase.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basal Ganglia Single Cell Validation Use Case\n",
    "\n",
    "***\n",
    "**Aim: ** To validate single cell models of the Basal Ganglia against data from literature.\n",
    "\n",
    "***\n",
    "**Version:** 3.1 (14/01/2020)\n",
    "***\n",
    "**Contributors:**  Shailesh Appukuttan (CNRS)\n",
    "***\n",
    "**Contact:** [shailesh.appukuttan@unic.cnrs-gif.fr](mailto:shailesh.appukuttan@unic.cnrs-gif.fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\" style=\"color:black\">\n",
    "<h2>About This Use Case</h2><br />\n",
    "This test shall take as input a BluePyOpt optimized output file, containing a `hall_of_fame.json` file specifying a collection of parameter sets. The validation test would then evaluate the model for all (or specified) parameter sets against various eFEL features. The results are registered on the HBP Validation Framework app. If an instance of the Model Catalog and Validation Framework are not found in the current Collab, then these are created. Additionally, a test report is generated and this can be viewed within the Jupyter notebook, or downloaded.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Seting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the proper version of NEURON\n",
    "!ln -sfn /home/jovyan/.local/nrn-7.6/ /home/jovyan/.local/nrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pkg_resources\n",
    "from pkg_resources import parse_version\n",
    "!pip install -q tornado==4.5.3\n",
    "!pip install -q --upgrade hbp-service-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Note:</strong> If you encounter any errors in the below cell, please try to restart the kernel (Kernel -> Restart & Run All)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_packages = [    \n",
    "                    {\"sciunit\"                  : {\"min_version\": \"0.2.1\",  \"install_version\": \"0.2.1\"}},\n",
    "                    {\"bluepyopt\"                : {\"min_version\": \"1.6.22\", \"install_version\": \"1.6.42\"}},\n",
    "                    {\"basalunit\"                : {\"min_version\": \"1.0.4\",  \"install_version\": \"1.0.4\"}},\n",
    "                    {\"hbp_validation_framework\" : {\"min_version\": \"0.5.28\", \"install_version\": \"0.5.28\"}},\n",
    "                    {\"numpy\"                    : {\"min_version\": \"1.16.2\", \"install_version\": \"1.16.2\"}},\n",
    "                    {\"Jinja2\"                   : {\"min_version\": \"2.10.3\", \"install_version\": \"2.10.3\"}},\n",
    "                    {\"tornado\"                  : {\"min_version\": \"4.5.3\",  \"install_version\": \"4.5.3\"}}\n",
    "                ]\n",
    "\n",
    "def install_req_packages():\n",
    "    # currently handles installations via PyPI and GitHub\n",
    "    for pkg in req_packages:        \n",
    "        for pkg_name, pkg_vinfo in pkg.items():\n",
    "            print(\"Checking for package: {}\".format(pkg_name))        \n",
    "            try:\n",
    "                pkg_resources.get_distribution(pkg_name)        \n",
    "                current_version = parse_version(pkg_resources.get_distribution(pkg_name).version)\n",
    "                print(\"\\t{}: current version = {}\".format(pkg_name, current_version))\n",
    "                if not pkg_vinfo[\"min_version\"] or current_version < parse_version(pkg_vinfo[\"min_version\"]) or current_version > parse_version(pkg_vinfo[\"install_version\"]):                                                \n",
    "                        print(\"\\tInstalling another version of {}.\".format(pkg_name))\n",
    "                        raise\n",
    "            except:            \n",
    "                if \"github.com\" in pkg_vinfo[\"install_version\"]:\n",
    "                    os.system(\"pip install --quiet --no-cache-dir --force-reinstall git+{}\".format(pkg_vinfo[\"install_version\"]))\n",
    "                else:\n",
    "                    os.system(\"pip install --quiet --no-cache-dir --force-reinstall {}=={}\".format(pkg_name, pkg_vinfo[\"install_version\"]))                                \n",
    "                print(\"\\t{}: installed version = {}\".format(pkg_name, pkg_vinfo[\"install_version\"]))                \n",
    "\n",
    "install_req_packages()                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import tarfile\n",
    "import urllib\n",
    "import sciunit\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import bluepyopt.ephys as ephys\n",
    "from basalunit.utils import CellModel\n",
    "from hbp_validation_framework import utils, TestLibrary, ModelCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check if Model Catalog and Validation Framework Apps Exist in Collab\n",
    "If the notebook is run inside a Collab, we check if an instance of the Model Catalog and Validation Framework apps exist in the current Collab. If not, we add an instance of each (this will be reflected in the Collab's navigation panel, possibly on reloading the page).\n",
    "\n",
    "NOTE: **HBP_USERNAME** is an optional parameter when the notebook is being run inside the Collaboratory. The notebook can automatically identify your username in this scenario. This parameter needs to be specified if a user wishes to download the notebook and run it locally. Another potential (less likely) reason for specifying this (even within the Collaboratory) is in dealing with access permissions (wanting to run the test with different credentials).\n",
    "\n",
    "NOTE: Even if this notebook is not run inside a Collab, the following cell needs to be executed. It will identify the environment and manage accordingly. When not run inside a Collab, it will simply setup parameters required for the test, and not attempt to create new apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your HBP username; not essential if running inside the Collaboratory\n",
    "HBP_USERNAME = \"\"\n",
    "testLibrary = TestLibrary(username=HBP_USERNAME)\n",
    "modelCatalog = ModelCatalog.from_existing(testLibrary)\n",
    "\n",
    "try:\n",
    "    collab_path = get_collab_storage_path()\n",
    "    collab_id = collab_path[1:] # this might fail for very old Collabs which use name instead of Collab ID\n",
    "except:\n",
    "    # not run inside Collaboratory\n",
    "    print(\"\\nPlease enter a Collab ID where you wish to store the results:\")\n",
    "    print(\"E.g.: 8123\")\n",
    "    print(\"Note: you should be a member of this Collab!\")\n",
    "    collab_id = input()\n",
    "    if not isinstance(collab_id, int):\n",
    "        raise ValueError(\"Possibly invalid Collab ID: {}. Numeric input expected!\".format(collab_id))    \n",
    "\n",
    "# check if apps exist; if not then create them\n",
    "MCapp_navID = modelCatalog.exists_in_collab_else_create(collab_id)\n",
    "modelCatalog.set_app_config(collab_id=collab_id, app_id=MCapp_navID, only_if_new=\"True\")\n",
    "VFapp_navID = testLibrary.exists_in_collab_else_create(collab_id)\n",
    "testLibrary.set_app_config(collab_id=collab_id, app_id=VFapp_navID, only_if_new=\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection: Specifying model from ModelCatalog\n",
    "The user is given an option to choose from existing compatible models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = modelCatalog.get_model(model_id=\"c27e711e-75e3-498f-bc64-6242ea5d8772\") # d9c2404f-427f-4205-8aa3-6989131ce96e\n",
    "m2 = modelCatalog.get_model(model_id=\"6deb0c49-e762-4b60-8206-a46322bca3db\") # 36c1dcb2-779c-4b44-b842-ecb255a3fbbe\n",
    "m3 = modelCatalog.get_model(model_id=\"0b2fda97-2403-482b-90d1-738d1b6d013c\") # c1c513e3-8ada-4091-ba3a-22537ccf05eb\n",
    "list_of_models = [m1, m2, m3]\n",
    "df = pd.DataFrame.from_dict(json_normalize(list_of_models), orient='columns')\n",
    "df = df.reindex(columns=['name', 'id', 'author', 'brain_region', 'species', 'cell_type', 'model_scope', 'abstraction_level', 'description'])\n",
    "df.index += 1 \n",
    "print(\"Available models are listed below:\")\n",
    "df.replace('\\n','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter the # of required model: \")\n",
    "choice = int(input())\n",
    "if choice <= len(list_of_models) and choice > 0:   \n",
    "    model_id = list_of_models[choice-1][\"id\"]\n",
    "    model_name = list_of_models[choice-1][\"name\"]\n",
    "else:\n",
    "    raise ValueError(\"Invalid entry for model choice!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gather Additional Info\n",
    "Currently the test is applicable for three cell types: _D1-type MSN_, _D2-type MSN_ and _FS Interneurons_ <br />\n",
    "The usecase tries to identify the cell type from the model name. If this isn't possible, the user is prompted to specify the cell type.\n",
    "\n",
    "Experimental recordings might specify a value for their junctional potential. For the observation data used here, the same was recorded as 9.5 mV and is therefore used as the default value. The user is prompted to change this if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will try to determine cell type from model name; if not then ask user\n",
    "# current options: msn_d1, msn_d2, fs\n",
    "if \"msn_d1\" in model_name:\n",
    "    cell_type = \"msn_d1\" \n",
    "elif \"msn_d2\" in model_name:\n",
    "    cell_type = \"msn_d2\"\n",
    "elif \"-fs-\" in model_name:\n",
    "    cell_type = \"fs\"    \n",
    "else:\n",
    "    print(\"\\nPlease enter the cell_type: \")\n",
    "    options = [\"msn_d1\", \"msn_d2\", \"fs\"]\n",
    "    for i, each in enumerate(options,start=1):\n",
    "        print(\"\\t{}. {}\".format(i,each))\n",
    "    print(\"Enter the # of cell_type: \")\n",
    "    choice = int(input())\n",
    "    cell_type = options[choice-1]\n",
    "print(\"Cell Type = {}\".format(cell_type))\n",
    "    \n",
    "print(\"\\nEnter a value for junction potential of experimental recordings (in mV):\")\n",
    "print(\"Note: leave blank to use default value (i.e. 9.5); enter numeric value, no units\")\n",
    "jp_value = input()\n",
    "if jp_value == \"\":\n",
    "    junction_potential = 9.5 # mV\n",
    "else:\n",
    "    junction_potential = float(jp_value)\n",
    "print(\"Junction Potential = {} mV\".format(junction_potential))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Selection of desired model instance\n",
    "The model has several model instances corresponding to the Hall of Fame (HoF) parameter sets found in the model's \"hall_of_fame.json\" file. These are listed below. The user needs to select one of these model instances.\n",
    "\n",
    "The model morphology file (.swc) that was found within the model directory, and used to construct the model, is indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instances = modelCatalog.list_model_instances(model_id=model_id)\n",
    "df = pd.DataFrame.from_dict(json_normalize(model_instances), orient='columns')\n",
    "df = df.reindex(columns=['version', 'id', 'parameters'])\n",
    "df.index += 1\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.replace('\\n','', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user is asked to specify the model instances that they are interested in validating. The valid range of values are indicated. The user can provide input as follows: <br />\n",
    " * All parameters, specify: `all` <br />\n",
    " * Single parameter set, specify number, e.g.: `1` <br />\n",
    " * Multiple parameter sets, specify numbers as a list, e.g.: `[1,4,5,8]` <br />\n",
    " \n",
    "**Note:** validation of each parameter set takes around 20 mins; for testing specify a single parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enter the model instance(s) to be validated: 1 - {}\".format(len(model_instances)))\n",
    "print(\"Example inputs: 1, 4, [1,4,5,8], all\")\n",
    "instances_entry = input().lower()\n",
    "if instances_entry == \"all\":\n",
    "    instances_list = range(1, len(model_instances)+1)\n",
    "else:    \n",
    "    if isinstance(eval(instances_entry), list):\n",
    "        instances_list = eval(instances_entry)        \n",
    "    elif isinstance(eval(instances_entry), int):\n",
    "        instances_list = [eval(instances_entry)]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid entry for parameter set!\")  \n",
    "       \n",
    "valid_instances_list = []\n",
    "for i in instances_list:\n",
    "    if i > 0 and i <= len(model_instances):\n",
    "        valid_instances_list.append(i)\n",
    "    else:\n",
    "        print(\"Invalid entry: {}. Excluded.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Note: For all models used in this UseCase, all the model instances have same source code\n",
    "    # They differ only in the parameter values assigned\n",
    "    file_path = model_instances[0][\"source\"]\n",
    "    file_tmp = urllib.request.urlretrieve(file_path, filename=os.path.basename(file_path))[0]\n",
    "    file_path = os.path.join(os.getcwd(),os.path.basename(file_path))\n",
    "except:\n",
    "    raise IOError(\"Model url = {} is invalid!\".format(file_path))    \n",
    "    \n",
    "cell_model = CellModel(model_path=file_path, cell_type=cell_type, model_name=model_name)\n",
    "cell_model.model_uuid = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiating the model(s); Running Model Validation Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It takes roughly 20 minutes for each model instance. At the end of the test, the user is provided with a textual summary of the _score_ and the path to related output files generated by the test. These and other details can be viewed in the  _Validation Framework_ app (see Collab's Navigation panel; select `Validation Framework`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uuids = []\n",
    "\n",
    "for index in valid_instances_list:\n",
    "    cell_model.model_version = model_instances[index-1][\"version\"]\n",
    "    cell_model.params = ast.literal_eval(model_instances[index-1][\"parameters\"])\n",
    "    \n",
    "    print(\"NOTE: The simulation(s) may take some time to complete (typically 10-15 minutes). Please wait.\")\n",
    "    result_id, score = utils.run_test(username=HBP_USERNAME, model=cell_model, test_alias=\"basalg_\" + cell_type, test_version=\"1.0\", storage_collab_id=collab_id, register_result=True, client_obj=testLibrary, cell_type=cell_type, junction_potential=junction_potential, use_cache=True)\n",
    "    result_uuids.append(result_id)    \n",
    "\n",
    "print(\"The result(s) can be viewed in the HBP Validation Framework app. Direct link(s):\")\n",
    "for result_uuid in result_uuids:\n",
    "    print(\"https://collab.humanbrainproject.eu/#/collab/{}/nav/{}?state=result.{}\".format(str(collab_id),str(VFapp_navID), result_uuid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Score Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(result_uuids) > 0:\n",
    "    df, excluded_results = utils.generate_score_matrix(result_list=result_uuids, collab_id=collab_id, client_obj=modelCatalog)        \n",
    "    from IPython.core.display import HTML\n",
    "    HTML(\"<style>.rendered_html th {max-width: 120px;}</style>\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate Report\n",
    "The validation framework can generate an HTML report for all the successfully completed tests. The user is prompted whether such a report should be generated for the current validation results. If asked to generate, the location to the generated HTML is indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = None\n",
    "if len(result_uuids) > 0:\n",
    "    print(\"\\nDo you wish to generate an HTML report of the executed tests?\")\n",
    "    print(\"Enter: y/n\")\n",
    "    choice = input().lower()\n",
    "    valid_choices = {\"yes\": True, \"y\": True, \"no\": False, \"n\": False}\n",
    "    if valid_choices[choice]:\n",
    "        report_path, valid_uuids = utils.generate_HTML_report(result_list=result_uuids, collab_id=collab_id, client_obj=modelCatalog)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 View Report Inside Jupyter Notebook\n",
    "The HTML report created in the above cell is displayed within the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_path:\n",
    "    rel_report_path = os.path.relpath(report_path)\n",
    "    from IPython.display import IFrame    \n",
    "    display(IFrame(rel_report_path, width=\"100%\", height=1000))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
